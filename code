client = pymongo.MongoClient("mongodb+srv://raghulpachayappan:pcdk0303@cluster0.6anqyvx.mongodb.net/?retryWrites=true&w=majority",)
db = client.twitterscraping
record=db.scraped_data



twt_list = []


choice = st.sidebar.selectbox('Menu',["Home","About","Search","Display","Download"])


if choice == "Home":
    st.write('''This app is a Twitter Scraping web app created using Streamlit.
             It scrapes the twitter data for the given hashtag/ keyword for the given period.
             The tweets are uploaded in MongoDB and can be dowloaded as CSV or a JSON file.''')

elif choice=="About":
     st.write('''Twitter Scraper will scrape the data from Public Twitter profiles.
                    It will collect the data about **date, id, url, tweet content, users/tweeters,reply count, 
                    retweet count, language, source, like count, followers, friends** and lot more information 
                    to gather the real facts about the Tweets.''')


elif choice=="Search":

    
    clear = record.delete_many({})

    item = st.selectbox('Through which way you wanted to get search ?',['Hashtag','username'])
    st.write('you have selected', item)
    items = ['Hashtag','username']

  
    d1 = st.date_input('Enter the start date:',datetime.date(2022, 1, 1))
    st.write('Tweets starts from', d1)

    d2 = st.date_input('Enter the end date:',datetime.date(2023, 3, 1))
    st.write('Tweets ended at', d2)

    if item in items:
        search_text = st.text_input('Enter the text:')
        date_range = ' since:'+d1.strftime("%Y-%m-%d")+' until:'+d2.strftime("%Y-%m-%d")
        query= search_text+date_range
        number = st.number_input('Enter the number of tweets you wanted to scrape',1,1000)
        st.write(query)

        scraper = sntwitter.TwitterSearchScraper(query)
        st.write(scraper)
        for i, tweet in enumerate(scraper.get_items()):
            if i >(number - 1):
                break
            twt_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])

        
        df1 = pd.DataFrame(twt_list, columns=['Datetime','Tweet Id','Text', 'Username'])

        
        tweet_data = json.loads(df1.to_json(orient='records'))
        record.insert_many(list(tweet_data))
        st.write('Tweets successfully scraped')

elif choice=="Display":
    df2 = pd.DataFrame(list(record.find()))
    st.dataframe(df2)


else:
   col1, col2 = st.columns(2)
   with col1:
       st.write("Download the tweet data as CSV File")
       df = pd.DataFrame(list(record.find()))
       df.to_csv('twittercsv.csv')


       def convert_df(data):
           return df.to_csv().encode('utf-8')


       csv = convert_df(df)
       st.download_button(
           label="Download data as CSV",
           data=csv,
           file_name='twtittercsv.csv',
           mime='text/csv',
       )
       st.success("Successfully Downloaded data as CSV")


   with col2:
       st.write("Download the tweet data as JSON File")
       twtjs = df.to_json(default_handler=str).encode()
       obj = json.loads(twtjs)
       js = json.dumps(obj, indent=4)
       st.download_button(
           label="Download data as JSON",
           data=js,
           file_name='twtjs.js',
           mime='text/js',
       )
       st.success("Successfully Downloaded data as JSON")
